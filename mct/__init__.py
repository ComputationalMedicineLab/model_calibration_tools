"""
Model Calibration Tools
"""
import numpy as np
from sklearn.base import clone
from sklearn.neighbors import KernelDensity


def histograms(probs, actual, bins=100):
    """
    Calculates two histograms over [0, 1] by partitioning `probs` with `mask`
    and sorting each partition into `bins` sub-intervals.
    """
    actual = actual.astype(np.bool)
    edges, step = np.linspace(0., 1., bins, retstep=True, endpoint=False)
    idx = np.digitize(probs, edges) - 1
    top = np.bincount(idx, weights=actual, minlength=bins)
    bot = np.bincount(idx, weights=(~actual), minlength=bins)
    return top, bot, edges, step


def draw_histograms(probs, actual, ax=None, bins=100):
    """
    Plots the two histograms generated by ``histograms``; the falsey histogram
    is plotted underneath the x axis while the truthy histogram is plotted
    above.
    """
    # Import plt here so it don't interfere with other functionality
    import matplotlib.pyplot as plt

    if ax is None:
        ax = plt.gca()

    top, bot, edges, step = histograms(probs, actual, bins=bins)

    ax.axhline(linestyle='dashed', color='black', alpha=0.2)
    ax.bar(edges, top, width=step)
    ax.bar(edges, -bot, width=step)
    height = max(abs(x) for x in ax.get_ylim())
    ax.set_ylim([-height, height])
    return ax


def kde_calibration_curve(probs, actual,
                          bins=100,
                          kernel='gaussian',
                          bandwidth=0.1,
                          retraw=False):
    """
    Generate a calibration curve smoothed via KDE.
    """
    kde = KernelDensity(kernel=kernel, bandwidth=bandwidth)
    x_axis = np.linspace(0, 1, bins)
    y_true = _score(kde, probs[actual], x_axis)
    y_total = _score(kde, probs, x_axis)
    curve = y_true / y_total
    if retraw:
        return (curve, y_true, y_total)
    else:
        return curve


def _score(kde, train, x_axis):
    kde = clone(kde)
    kde.fit(train.reshape(-1, 1))
    scores = kde.score_samples(x_axis.reshape(-1, 1))
    # Scores has been normalized so that the area under np.exp(scores) is equal
    # to one, but since we're using this to generate another curve, we need to
    # de-normalize the curve
    return np.exp(scores) * len(train)
